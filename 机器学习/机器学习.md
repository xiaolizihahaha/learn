# 机器学习

### 1.python相关的库

- 科学函数库

  Scipy、numpy

- 绘图库

  matplotlib

- 数据分析库

  pandas

- 

### 2.概念

- 模型/学习器：计算机层面的认知

- 实例/样本： 对于某个对象的描述

- 属性空间/样本空间/输入空间：属性张成的空间

- 特征向量（feature vector）: 在属性空间里每个点对应一个坐标向量，**把一个示例称作特征向量**

- 维数（dimensionality）: 描述样本参数的个数（也就是空间是几维的）

- 正类（positive class）: 二分类里的一个

- 反类（negative class）: 二分类里的另外一个

  

- 训练集：学习样本数据集，通过**匹配一些参数来建立一个模型**，主要用来训练模型。类比考研前做的解题大全。

- 验证集：**对学习出来的模型，调整模型的参数**，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。

- 测试集：**测试训练好的模型的分辨能力**。类比 考研。这次真的是一考定终身。

  

- 正确率 —— 提取出的正确信息条数 / 提取出的信息条数

- 召回率 —— 提取出的正确信息条数 / 样本中的信息条数

- F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）

  ```
  举个例子如下: 某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。
  现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只 虾， 100 只乌龟。
  
  那么这些指标分别如下: 
  正确率 = 700 / (700 + 200 + 100) = 70% 
  召回率 = 700 / 1400 = 50% 
  F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%
  ```

  

- 学习率：步长

  

### 3.KNN

- **概念**

  k 近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。**k 近邻算法假设给定一个训练数据集，其中的实例类别已定。**分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过**多数表决**等方式进行预测。

- **原理**

  1. 假设有一个**带有标签的样本数据集**（训练样本集），其中包含每条数据与所属分类的对应关系。
  2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
     1. **计算新数据与样本数据集中每条数据的距离。**
     2. 对求得的所有距离进行排序（从小到大，越小表示越相似）。
     3. **取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。**
  3. **求 k 个数据中出现次数最多的分类标签作为新数据的分类。**

- **例子**：KNN手写数字（.txt）、约会对象选择

  

- **代码步骤**

  1. 创建两个变量features、labels。
     - 其中features为一个矩阵，用来装全部样本的各个属性值；
     - labels为一个列表，用来装全部样本的各个标签值。
  2. 数据处理：将每个样本整合成矩阵features的某一行，行中存着各个样本的属性值；将各个样本的标签值存入labels中。
  3. （可以利用测试集测试错误率）
  4. 预测一条新数据的标签：
     - 新数据的属性值存在变量newdata中，newdata相当于fertures中的一行；
     - 遍历features中的各行，计算新数据与各行之间的距离；
     - 根据距离从小到大对各行进行排序（一般是返回排好序后的index，indexs = distance.argsort() ），从而得到排名前k条数据的label值，然后少数服从多数，作为新数据的label值。

### 4.决策树

- **概念**

  1. **决策树**模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。（决策树学习通常包括 3 个步骤: 特征选择、决策树的生成和决策树的修剪。

  2. 决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。

  3. **熵（entropy）**: 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。

  4. **信息论（information theory）中的熵（香农熵）**: 是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。

  5. **信息增益（information gain）**: 在划分数据集前后信息发生的变化称为信息增益。

     

- 如何计算香农熵

  一个数据集的香农熵，是根据label（最后分类的标签）将数据集划分为几类，然后计算各类的prob（该类在数据集中所占的比例）。（prob为公式中的p(x)）

  ![image-20231010100249732](/Users/lifangyuan/Library/Application Support/typora-user-images/image-20231010100249732.png)

  

- 将一个属性选定为一个决策后的香农熵如何计算？

  得到该属性下各个可能的值values列表，并以不同的value值为依据，将数据集再分为A、B、C...几种，其中percent表示A、B、C这几种可能分别占values总数的多少。则 将一个属性选定为一个决策后的香农熵 = percent1 * H1 + percent2 * H2 + percent3 * H3（其中H3表示香农熵为 value值都为C的除去该属性列的其他属性组成的数据集）

  

- 代码步骤

  1. 创建数据集，其中包括两个变量：

     - 一个是用于存储样本的**属性值和标签值**的矩阵 dataset；
     - 另一个是用于存储样本属性名的列表features。

  2. 创建决策树createTree（dataset，features）

     - 如果发现dataset的最后一列（标签值）只有一种可能，那么返回该标志值。
     - 如果发现dataset的各个属性已经被分发完毕了，即发现dataset只剩一个列（label）了，那么将该列下的各个值进行vote，少数服从多数，作为返回值。
     - 遍历属性列表features，**得到最利于分类的一个属性下标i**，得到该属性的名称bestFeature = label[i]；
     - 从features中删除最利于分类的属性 del(features[i])；
     - 创建空树 ： mytree = {bestFeature:{}}
     - 得到最利于分类的属性下对应的所有value值，并遍历value值进行 建树myTree[bestFeature] [value] =  createTree（newDataset，newFeatures）;（**其中newDataset为切割后的新树**，newFeatures为切除掉该最优属性后的属性列表。
     - 返回决策树myTree

  3. 通过决策树进行分类 classify(treeModel,features,testData)

     - 首先找到决策树的根节点（某个特征），并获取其两个子树。

     - 在features列表中找到该对应于该根节点的index，进而在testData中找到对应于该属性值的value。

     - 找到value后，便可根据value确定选择哪个子树，secondFeature = seconds[value]

     - 然后判断子树secondFeature是否为dict类型，如果是，则继续分类classLabel = classify(secondFeature,features,testData)；如果不是，则classLabel = secondFeature

     - 返回classLabel

       ![image-20231010112110921](/Users/lifangyuan/Library/Application Support/typora-user-images/image-20231010112110921.png)

       <img src="/Users/lifangyuan/Library/Application Support/typora-user-images/image-20231010112041417.png" alt="image-20231010112041417" style="zoom:50%;" />

       

     - 补充：

       

       1. **newDataset为切割后的新树**：splitDataSet(dataset, index, value):

          对于一个dataset，要找到第index列 所有值为value的数据，找到之后形成数据集set1，在set1中删除掉第index列，由其他列组成新的数据集。

          ```python
          def splitDataSet(dataset, index, value):
           
            newDataset = []
            
            for data in dataset:
              if data[index] == value:
              	splitdata = data[:index]
              	splitdata.extend(data[index+1 : ])
             		newDataset.append(splitdata)
            
            return newDataset
          ```

          

       2. **得到最利于分类的一个属性下标i**：

          - 在dataset中 除去最后一行label列，遍历其他的属性列；
          - 分别计算各个属性选定为决策后的香农熵（如何计算见上）
          - 在其中选定一个相对于原dataset，相减值最大（dataset熵-选定属性后的熵）的属性index，并返回

       

### 5.朴素贝叶斯(基于概率类的分类方法)(每个词出现与否和其他词没有关系，如“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。)

- 依据贝叶斯定理进行分类的算法叫做贝叶斯算法

- **一般概率：**用 p1(x,y) 表示数据点 (x,y) 属于类别 1的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别:

  - 如果 p1(x,y) > p2(x,y) ，那么类别为1
  - 如果 p2(x,y) > p1(x,y) ，那么类别为2

  也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

- **条件概率：**P(AB) = P(B) * P(A|B) = P(A) * P(B|A)

  

- 真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到:

  [![应用贝叶斯准则](https://github.com/apachecn/ailearning/raw/master/docs/ml/img/NB_5.png)](https://github.com/apachecn/ailearning/blob/master/docs/ml/img/NB_5.png)
  
  
  
  使用上面这些定义，可以定义贝叶斯分类准则为:
  
  - 如果 P(c1|x, y) > P(c2|x, y), 那么属于类别 c1;
  
  - 如果 P(c2|x, y) > P(c1|x, y), 那么属于类别 c2.
  
  
  
- 贝叶斯算法常用做**文档自动分类**，将一个文档作为一个实例，将文档中某些词作为特征，而每个词的出现或者不出现（或者每个词出现的次数）作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。前一种实现方式并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。

  ## 

- 公式：P(AB) = P(B) * P(A|B) = P(A) * P(B|A)

  P(属于类别|具有某特征) = P(具有某特征|属于类别) * P(属于类别) / P(具有某特征) 

  ![image-20231016103113564](/Users/lifangyuan/Library/Application Support/typora-user-images/image-20231016103113564.png)

  

- **问: 上述代码实现中，为什么没有计算P(具有某特征)？**

  答: 根据上述公式可知，我们右边的式子等同于左边的式子，由于对于一个实例（一篇文档），**P(具有某特征)是固定的。**并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。
  
  首先计算概率 p(属于某类别) 。接下来计算 p(**具有某特征** | 属于某类别) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2...wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用 **p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci) 来计算上述概率**，这样就极大地简化了计算的过程。
  
  
  
  - **PS1**：在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，**可以将所有词的出现数初始化为 1，并将分母初始化为 2** （取1 或 2 的目的主要是为了保证分子和分母不为0，可以根据业务需求进行更改）。
  
  - **PS2**：另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是**对乘积取自然对数**。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以**避免下溢出**或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。
  
    于是就有：**P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))**



- 代码步骤：

  - 拿到一个用作训练的数据集，包括一个矩阵postingList（里面存放了每篇文档的词条收集数据）和一个列表classVec（里面存放了每篇文档的类别）
  - 统计全局数据集中词条的集合，将此集合转为列表vocabList（统计有哪些词条）
  - 遍历训练数据集中的每个文档，统计该文档中有没有每个词条，有的话记为1，没有的话记为0。那么对于每个文档得到一行数据（长度为len(vocabList)，类似于[0, 1, 0, 0, 1, 1, 0]），对于整个数据集得到一个 m * n 矩阵trainData。
  - 遍历trainMatrix，对每个类别的各个词条进行计数（需要注意的是计数初始化不要从0开始，可以从1开始，防止后面连乘等于0），进而可以算出每个类别中各个词条的概率（算出概率后，可以对概率进行取对数操作，这样可以防止后面连乘结果过于小导致四舍五入为0）。至此可以得到每个类别关于各个词条出现概率的对数列表p0Matrix、p1Matrix......（[log(P(F1|C1)),log(P(F2|C1)),log(P(F5|C1))....]）。除此之外，我们还可以得到各个类别的概率p0、p1...（属于该类别的文档数 / 总文档数）
  - 最后我们利用测试数据（单篇文档）的testData（长度为len(vocabList)，类似于[0, 1, 0, 0, 1, 1, 0]），与单个类别关于各个词条出现概率的对数列表p0Matrix相乘（列表各位相乘），并将该结果列表求和，求和后与该类别的概率p0相乘，得到一个结果值res0，其他类别也这样操作得到res1、res2...，选取一个最大值，则得到了与该文档最相似的一个文档，将最相似的文档的类别作为该文档的类别

- 

  

### 6.Logistic回归

- 

### 7.

- 

  